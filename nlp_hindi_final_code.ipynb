{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "448f5ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (79.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afc76ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fe9c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b35f5a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (2.5.1)\n",
      "Requirement already satisfied: portalocker in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: regex in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from sacrebleu) (2.2.5)\n",
      "Requirement already satisfied: colorama in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from sacrebleu) (5.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac472414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e11e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f9ff4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (2.2.5)\n",
      "Requirement already satisfied: dill in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (3.11.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c744865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (79.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d48b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (2.2.5)\n",
      "Requirement already satisfied: pandas in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./nlp_hindi_project/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37751165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2164\u001b[39m\n\u001b[32m   2160\u001b[39m         app.demo()\n\u001b[32m   2163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2164\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2141\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2138\u001b[39m config = HindiNLPConfig(\u001b[33m\"\u001b[39m\u001b[33mconfig.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2140\u001b[39m \u001b[38;5;66;03m# Set up logging\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2141\u001b[39m logger = \u001b[43mHindiNLPLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2142\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mStarting Hindi NLP Framework\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2144\u001b[39m \u001b[38;5;66;03m# Initialize main application\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1902\u001b[39m, in \u001b[36mHindiNLPLogger.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m   1900\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize logger with configuration.\"\"\"\u001b[39;00m\n\u001b[32m   1901\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m-> \u001b[39m\u001b[32m1902\u001b[39m \u001b[38;5;28mself\u001b[39m.logger = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1921\u001b[39m, in \u001b[36mHindiNLPLogger._setup_logger\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1919\u001b[39m log_file = \u001b[38;5;28mself\u001b[39m.config.get(\u001b[33m\"\u001b[39m\u001b[33mlogging.file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m log_file:\n\u001b[32m-> \u001b[39m\u001b[32m1921\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1922\u001b[39m     file_handler = logging.FileHandler(log_file)\n\u001b[32m   1923\u001b[39m     file_handler.setLevel(level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:227\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, \n",
    "    AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
    "    DataCollatorForSeq2Seq, DataCollatorWithPadding\n",
    ")\n",
    "from sacrebleu import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "#################################\n",
    "# 1. MACHINE TRANSLATION MODULE #\n",
    "#################################\n",
    "class HindiTranslationModel:\n",
    "    def __init__(self, model_name=\"ai4bharat/IndicBART\", max_length=128):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def preprocess_translation_data(self, examples, src_lang=\"en\", tgt_lang=\"hi\"):\n",
    "        \"\"\"Tokenize and prepare inputs for translation model.\"\"\"\n",
    "        inputs = [example for example in examples[src_lang]]\n",
    "        targets = [example for example in examples[tgt_lang]]\n",
    "        \n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs, \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                targets, \n",
    "                max_length=self.max_length, \n",
    "                padding=\"max_length\", \n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "            \n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "    \n",
    "    def load_iitb_corpus(self, path_to_data, sample_size=None):\n",
    "        \"\"\"Load and preprocess the IIT Bombay English-Hindi Parallel Corpus.\"\"\"\n",
    "        # For demonstration, using a simpler approach - in production, use proper data loading\n",
    "        if os.path.exists(path_to_data):\n",
    "            df = pd.read_csv(path_to_data, sep='\\t', names=['en', 'hi'])\n",
    "            if sample_size:\n",
    "                df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            # Create a small demo dataset if file doesn't exist\n",
    "            print(f\"Warning: {path_to_data} not found. Creating a small demo dataset.\")\n",
    "            data = {\n",
    "                'en': [\"Hello, how are you?\", \"Where is the library?\", \"I like Indian food.\"],\n",
    "                'hi': [\"नमस्ते, आप कैसे हैं?\", \"पुस्तकालय कहां है?\", \"मुझे भारतीय खाना पसंद है।\"]\n",
    "            }\n",
    "            df = pd.DataFrame(data)\n",
    "        \n",
    "        # Split into train-val-test\n",
    "        train_df = df.sample(frac=0.8, random_state=42)\n",
    "        temp_df = df.drop(train_df.index)\n",
    "        val_df = temp_df.sample(frac=0.5, random_state=42)\n",
    "        test_df = temp_df.drop(val_df.index)\n",
    "        \n",
    "        # Convert to HF datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    def load_domain_specific_data(self, paths):\n",
    "        \"\"\"Load domain-specific parallel data for improved domain adaptation.\"\"\"\n",
    "        datasets = []\n",
    "        for domain, path in paths.items():\n",
    "            if os.path.exists(path):\n",
    "                df = pd.read_csv(path, sep='\\t', names=['en', 'hi'])\n",
    "                df['domain'] = domain\n",
    "                datasets.append(df)\n",
    "            else:\n",
    "                print(f\"Warning: {path} not found. Skipping {domain} dataset.\")\n",
    "        \n",
    "        if not datasets:\n",
    "            return None, None, None\n",
    "        \n",
    "        combined_df = pd.concat(datasets)\n",
    "        \n",
    "        # Split into train-val-test\n",
    "        train_df = combined_df.sample(frac=0.8, random_state=42)\n",
    "        temp_df = combined_df.drop(train_df.index)\n",
    "        val_df = temp_df.sample(frac=0.5, random_state=42)\n",
    "        test_df = temp_df.drop(val_df.index)\n",
    "        \n",
    "        # Convert to HF datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    def compute_metrics(self, eval_preds):\n",
    "        \"\"\"Calculate BLEU and METEOR scores for translation evaluation.\"\"\"\n",
    "        preds, labels = eval_preds\n",
    "        \n",
    "        # Replace -100 in the labels as we can't decode them\n",
    "        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
    "        \n",
    "        # Decode predictions and references\n",
    "        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu = corpus_bleu(decoded_preds, [decoded_labels]).score\n",
    "        \n",
    "        # Calculate METEOR score (sample for demonstration)\n",
    "        # In practice, calculate for all samples and average\n",
    "        meteor_scores = []\n",
    "        for pred, ref in zip(decoded_preds[:5], decoded_labels[:5]):  # Taking first 5 for demo\n",
    "            tokenized_pred = nltk.word_tokenize(pred)\n",
    "            tokenized_ref = nltk.word_tokenize(ref)\n",
    "            meteor_scores.append(meteor_score([tokenized_ref], tokenized_pred))\n",
    "        \n",
    "        meteor = np.mean(meteor_scores) if meteor_scores else 0\n",
    "        \n",
    "        return {\"bleu\": bleu, \"meteor\": meteor}\n",
    "    \n",
    "    def fine_tune(self, train_dataset, val_dataset, output_dir, epochs=3, batch_size=16):\n",
    "        \"\"\"Fine-tune the translation model on Hindi-English parallel data.\"\"\"\n",
    "        # Preprocess datasets\n",
    "        train_dataset = train_dataset.map(\n",
    "            lambda examples: self.preprocess_translation_data(examples),\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        )\n",
    "        val_dataset = val_dataset.map(\n",
    "            lambda examples: self.preprocess_translation_data(examples),\n",
    "            batched=True,\n",
    "            remove_columns=val_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            weight_decay=0.01,\n",
    "            save_total_limit=3,\n",
    "            num_train_epochs=epochs,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            report_to=\"tensorboard\",\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        trainer.train()\n",
    "        trainer.save_model(f\"{output_dir}/final_model\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def translate(self, text, src_lang=\"en\", tgt_lang=\"hi\"):\n",
    "        \"\"\"Translate text from source language to target language.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_length=self.max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation\n",
    "    \n",
    "    def batch_translate(self, texts, src_lang=\"en\", tgt_lang=\"hi\"):\n",
    "        \"\"\"Translate a batch of texts.\"\"\"\n",
    "        translations = []\n",
    "        for text in texts:\n",
    "            translation = self.translate(text, src_lang, tgt_lang)\n",
    "            translations.append(translation)\n",
    "        return translations\n",
    "    \n",
    "    def evaluate_idiomatic_expressions(self, idiomatic_dataset):\n",
    "        \"\"\"Specifically evaluate translation quality on idiomatic expressions.\"\"\"\n",
    "        sources = idiomatic_dataset['en']\n",
    "        references = idiomatic_dataset['hi']\n",
    "        \n",
    "        # Translate sources\n",
    "        translations = self.batch_translate(sources)\n",
    "        \n",
    "        # Compute metrics\n",
    "        bleu = corpus_bleu(translations, [references]).score\n",
    "        \n",
    "        # Compute METEOR for a sample\n",
    "        meteor_scores = []\n",
    "        for trans, ref in zip(translations[:20], references[:20]):  # Sample for demonstration\n",
    "            tokenized_trans = nltk.word_tokenize(trans)\n",
    "            tokenized_ref = nltk.word_tokenize(ref)\n",
    "            meteor_scores.append(meteor_score([tokenized_ref], tokenized_trans))\n",
    "        \n",
    "        meteor = np.mean(meteor_scores) if meteor_scores else 0\n",
    "        return {\"bleu\": bleu, \"meteor\": meteor}\n",
    "############################\n",
    "# 2. TEXT GENERATION MODULE #\n",
    "############################\n",
    "class HindiTextGenerator:\n",
    "    def __init__(self, model_name=\"ai4bharat/IndicBART\", max_length=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def preprocess_generation_data(self, examples, prompt_col=\"prompt\", completion_col=\"completion\"):\n",
    "        \"\"\"Preprocess data for text generation training.\"\"\"\n",
    "        model_inputs = self.tokenizer(\n",
    "            examples[prompt_col], \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = self.tokenizer(\n",
    "            examples[completion_col], \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "    \n",
    "    def load_hindi_generation_data(self, path):\n",
    "        \"\"\"Load data for Hindi text generation.\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            print(f\"Warning: {path} not found. Creating a small demo dataset.\")\n",
    "            data = {\n",
    "                'prompt': [\n",
    "                    \"हिंदी में एक कहानी लिखें जो एक किसान के जीवन के बारे में हो।\",\n",
    "                    \"भारत की संस्कृति के बारे में एक निबंध लिखें।\"\n",
    "                ],\n",
    "                'completion': [\n",
    "                    \"एक गाँव में एक किसान रहता था। उसका नाम राम था...\",\n",
    "                    \"भारत एक विविधताओं वाला देश है जहाँ अनेक संस्कृतियाँ मिलकर रहती हैं...\"\n",
    "                ]\n",
    "            }\n",
    "            df = pd.DataFrame(data)\n",
    "        \n",
    "        # Split into train-val\n",
    "        train_df = df.sample(frac=0.8, random_state=42)\n",
    "        val_df = df.drop(train_df.index)\n",
    "        \n",
    "        # Convert to HF datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def load_cultural_dataset(self, path):\n",
    "        \"\"\"Load culturally rich Hindi dataset for better cultural context.\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            print(f\"Warning: {path} not found. Creating a small demo cultural dataset.\")\n",
    "            data = {\n",
    "                'prompt': [\n",
    "                    \"होली के त्योहार पर एक छोटा निबंध लिखें।\",\n",
    "                    \"भारत के गणतंत्र दिवस के महत्व पर प्रकाश डालें।\"\n",
    "                ],\n",
    "                'completion': [\n",
    "                    \"होली रंगों का त्योहार है जो भारत में बड़े हर्षोल्लास के साथ मनाया जाता है...\",\n",
    "                    \"26 जनवरी, 1950 को भारत का संविधान लागू हुआ और भारत एक पूर्ण गणराज्य बना...\"\n",
    "                ]\n",
    "            }\n",
    "            df = pd.DataFrame(data)\n",
    "        \n",
    "        # Split into train-val\n",
    "        train_df = df.sample(frac=0.8, random_state=42)\n",
    "        val_df = df.drop(train_df.index)\n",
    "        \n",
    "        # Convert to HF datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def compute_perplexity(self, eval_dataset):\n",
    "        \"\"\"Compute perplexity on evaluation dataset.\"\"\"\n",
    "        eval_dataloader = DataLoader(\n",
    "            eval_dataset, \n",
    "            batch_size=8, \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_length = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in eval_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item() * input_ids.size(0)\n",
    "                total_length += input_ids.size(0)\n",
    "        \n",
    "        perplexity = torch.exp(torch.tensor(total_loss / total_length))\n",
    "        return perplexity.item()\n",
    "    \n",
    "    def fine_tune(self, train_dataset, val_dataset, output_dir, epochs=3, batch_size=8):\n",
    "        \"\"\"Fine-tune the text generation model.\"\"\"\n",
    "        # Preprocess datasets\n",
    "        train_dataset = train_dataset.map(\n",
    "            lambda examples: self.preprocess_generation_data(examples),\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        )\n",
    "        val_dataset = val_dataset.map(\n",
    "            lambda examples: self.preprocess_generation_data(examples),\n",
    "            batched=True,\n",
    "            remove_columns=val_dataset.column_names\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            weight_decay=0.01,\n",
    "            save_total_limit=3,\n",
    "            num_train_epochs=epochs,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            logging_dir=f\"{output_dir}/logs\"\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        trainer.train()\n",
    "        \n",
    "        # Compute perplexity\n",
    "        perplexity = self.compute_perplexity(val_dataset)\n",
    "        print(f\"Model perplexity: {perplexity}\")\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model(f\"{output_dir}/final_model\")\n",
    "        \n",
    "        return trainer, perplexity\n",
    "    \n",
    "    def generate_text(self, prompt, max_length=100, num_return_sequences=1):\n",
    "        \"\"\"Generate Hindi text based on prompt.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_texts = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        return generated_texts\n",
    "#################################\n",
    "# 3. SENTIMENT ANALYSIS MODULE #\n",
    "#################################\n",
    "class HindiSentimentAnalyzer:\n",
    "    def __init__(self, model_name=\"ai4bharat/MuRIL-base\", num_labels=3):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels\n",
    "        ).to(device)\n",
    "        self.id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "        self.label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    \n",
    "    def preprocess_sentiment_data(self, examples, text_col=\"text\", label_col=\"label\", max_length=128):\n",
    "        \"\"\"Preprocess data for sentiment analysis.\"\"\"\n",
    "        model_inputs = self.tokenizer(\n",
    "            examples[text_col], \n",
    "            max_length=max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        model_inputs[\"labels\"] = examples[label_col]\n",
    "        return model_inputs\n",
    "    \n",
    "    def load_hindi_sentiment_data(self, path):\n",
    "        \"\"\"Load Hindi sentiment analysis dataset.\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            print(f\"Warning: {path} not found. Creating a small demo sentiment dataset.\")\n",
    "            data = {\n",
    "                'text': [\n",
    "                    \"यह फिल्म बहुत अच्छी थी, मुझे बहुत पसंद आई।\",\n",
    "                    \"सेवा की गुणवत्ता औसत थी, कुछ खास नहीं।\",\n",
    "                    \"मैं इस उत्पाद से बिल्कुल संतुष्ट नहीं हूं, पैसे बर्बाद हो गए।\"\n",
    "                ],\n",
    "                'label': [2, 1, 0]  # positive, neutral, negative\n",
    "            }\n",
    "            df = pd.DataFrame(data)\n",
    "        train_df = df.sample(frac=0.7, random_state=42)\n",
    "        temp_df = df.drop(train_df.index)\n",
    "        val_df = temp_df.sample(frac=0.5, random_state=42)\n",
    "        test_df = temp_df.drop(val_df.index)\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    def load_domain_specific_sentiment_data(self, paths):\n",
    "        \"\"\"Load domain-specific sentiment datasets for domain adaptation.\"\"\"\n",
    "        datasets = []\n",
    "        for domain, path in paths.items():\n",
    "            if os.path.exists(path):\n",
    "                df = pd.read_csv(path)\n",
    "                df['domain'] = domain\n",
    "                datasets.append(df)\n",
    "            else:\n",
    "                print(f\"Warning: {path} not found. Skipping {domain} dataset.\")\n",
    "        \n",
    "        if not datasets:\n",
    "            return None, None, None\n",
    "        \n",
    "        combined_df = pd.concat(datasets)\n",
    "        \n",
    "        # Split into train-val-test\n",
    "        train_df = combined_df.sample(frac=0.7, random_state=42)\n",
    "        temp_df = combined_df.drop(train_df.index)\n",
    "        val_df = temp_df.sample(frac=0.5, random_state=42)\n",
    "        test_df = temp_df.drop(val_df.index)\n",
    "        \n",
    "        # Convert to HF datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    def compute_metrics(self, eval_preds):\n",
    "        \"\"\"Compute F1 score and other metrics for sentiment analysis.\"\"\"\n",
    "        predictions, labels = eval_preds\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        accuracy = (predictions == labels).mean()\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "    \n",
    "    def fine_tune(self, train_dataset, val_dataset, output_dir, epochs=5, batch_size=16):\n",
    "        \"\"\"Fine-tune sentiment analysis model.\"\"\"\n",
    "        # Preprocess datasets\n",
    "        train_dataset = train_dataset.map(\n",
    "            lambda examples: self.preprocess_sentiment_data(examples),\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        )\n",
    "        val_dataset = val_dataset.map(\n",
    "            lambda examples: self.preprocess_sentiment_data(examples),\n",
    "            batched=True,\n",
    "            remove_columns=val_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            weight_decay=0.01,\n",
    "            save_total_limit=3,\n",
    "            num_train_epochs=epochs,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            logging_dir=f\"{output_dir}/logs\"\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model(f\"{output_dir}/final_model\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def predict_sentiment(self, text):\n",
    "        \"\"\"Predict sentiment of Hindi text.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.softmax(outputs.logits, dim=1)\n",
    "            predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Get sentiment and confidence\n",
    "        sentiment = self.id2label[predicted_class]\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "        \n",
    "        return {\n",
    "            \"sentiment\": sentiment,\n",
    "            \"confidence\": confidence,\n",
    "            \"probabilities\": {\n",
    "                self.id2label[i]: prob.item() \n",
    "                for i, prob in enumerate(predictions[0])\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "##############################\n",
    "# 4. INTEGRATED PIPELINE    #\n",
    "##############################\n",
    "\n",
    "class HindiNLPPipeline:\n",
    "    def __init__(self):\n",
    "        self.translation_model = None\n",
    "        self.generation_model = None\n",
    "        self.sentiment_model = None\n",
    "    \n",
    "    def initialize_translation(self, model_name=\"ai4bharat/IndicBART\"):\n",
    "        \"\"\"Initialize translation model.\"\"\"\n",
    "        self.translation_model = HindiTranslationModel(model_name)\n",
    "        return self.translation_model\n",
    "    \n",
    "    def initialize_generation(self, model_name=\"ai4bharat/IndicBART\"):\n",
    "        \"\"\"Initialize text generation model.\"\"\"\n",
    "        self.generation_model = HindiTextGenerator(model_name)\n",
    "        return self.generation_model\n",
    "    \n",
    "    def initialize_sentiment(self, model_name=\"ai4bharat/MuRIL-base\"):\n",
    "        \"\"\"Initialize sentiment analysis model.\"\"\"\n",
    "        self.sentiment_model = HindiSentimentAnalyzer(model_name)\n",
    "        return self.sentiment_model\n",
    "    \n",
    "    def translate(self, text, src_lang=\"en\", tgt_lang=\"hi\"):\n",
    "        \"\"\"Translate text using the translation model.\"\"\"\n",
    "        if not self.translation_model:\n",
    "            self.initialize_translation()\n",
    "        return self.translation_model.translate(text, src_lang, tgt_lang)\n",
    "    \n",
    "    def generate(self, prompt, max_length=100):\n",
    "        \"\"\"Generate text using the generation model.\"\"\"\n",
    "        if not self.generation_model:\n",
    "            self.initialize_generation()\n",
    "        return self.generation_model.generate_text(prompt, max_length)\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analyze sentiment of text using the sentiment model.\"\"\"\n",
    "        if not self.sentiment_model:\n",
    "            self.initialize_sentiment()\n",
    "        return self.sentiment_model.predict_sentiment(text)\n",
    "    \n",
    "    def process_hinglish(self, text):\n",
    "        \"\"\"Process code-mixed Hinglish text.\"\"\"\n",
    "        roman_to_hindi = {\n",
    "            'namaste': 'नमस्ते',\n",
    "            'kaise': 'कैसे',\n",
    "            'ho': 'हो',\n",
    "            'aap': 'आप',\n",
    "            'main': 'मैं',\n",
    "            'hoon': 'हूँ',\n",
    "            # Add more mappings for common words\n",
    "        }\n",
    "        \n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.lower() in roman_to_hindi:\n",
    "                words[i] = roman_to_hindi[word.lower()]\n",
    "        \n",
    "        processed_text = ' '.join(words)\n",
    "        return processed_text\n",
    "    \n",
    "    def end_to_end_processing(self, text, task=\"all\"):\n",
    "        \"\"\"Perform end-to-end processing on input text.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Check if text is Hinglish and process if needed\n",
    "        if any(ord(c) < 128 for c in text) and any(ord(c) >= 128 for c in text):\n",
    "            text = self.process_hinglish(text)\n",
    "            results[\"processed_text\"] = text\n",
    "        \n",
    "        # Perform requested tasks\n",
    "        if task in [\"translate\", \"all\"]:\n",
    "            # Auto-detect language and translate to Hindi if not already in Hindi\n",
    "            is_hindi = all(ord(c) >= 128 for c in text if c.strip())\n",
    "            if not is_hindi:\n",
    "                translation = self.translate(text, src_lang=\"en\", tgt_lang=\"hi\")\n",
    "                results[\"translation\"] = translation\n",
    "        \n",
    "        if task in [\"generate\", \"all\"]:\n",
    "            # If text is a prompt, generate content based on it\n",
    "            generated_text = self.generate(text)\n",
    "            results[\"generated_text\"] = generated_text\n",
    "        \n",
    "        if task in [\"sentiment\", \"all\"]:\n",
    "            # Analyze sentiment\n",
    "            sentiment_result = self.analyze_sentiment(text)\n",
    "            results[\"sentiment_analysis\"] = sentiment_result\n",
    "        \n",
    "        return results\n",
    "#######################################\n",
    "# 5. SPECIALIZED CULTURAL ADAPTATION  #\n",
    "#######################################\n",
    "class CulturalAdaptation:\n",
    "    def __init__(self, base_model):\n",
    "        \"\"\"Initialize with a base model (translation, generation, or sentiment).\"\"\"\n",
    "        self.base_model = base_model\n",
    "        self.culture_specific_data = {}\n",
    "    \n",
    "    def load_cultural_expressions(self, path):\n",
    "        \"\"\"Load dataset of cultural expressions, idioms, and references.\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            self.culture_specific_data = df.to_dict('records')\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Warning: {path} not found. Cultural adaptation will be limited.\")\n",
    "            # Sample data\n",
    "            self.culture_specific_data = [\n",
    "                {\"expression\": \"लालन-पालन\", \"meaning\": \"upbringing\", \"context\": \"family\"},\n",
    "                {\"expression\": \"अतिथि देवो भव:\", \"meaning\": \"guest is god\", \"context\": \"hospitality\"}\n",
    "            ]\n",
    "            return False\n",
    "    \n",
    "    def enhance_translation(self, text, translation):\n",
    "        \"\"\"Enhance translation with cultural context.\"\"\"\n",
    "        enhanced = translation\n",
    "        for item in self.culture_specific_data:\n",
    "            if item[\"expression\"] in text:\n",
    "                # If cultural expression is found, ensure it's properly translated\n",
    "                if item[\"meaning\"] not in translation.lower():\n",
    "                    # This is simplified - in practice, would need more sophisticated replacement\n",
    "                    enhanced = enhanced + f\" [{item['expression']}: {item['meaning']}]\"\n",
    "        \n",
    "        return enhanced\n",
    "    \n",
    "    def enhance_text_generation(self, prompt, generated_text):\n",
    "        \"\"\"Enhance generated text with cultural context.\"\"\"\n",
    "        # Check if prompt involves cultural themes\n",
    "        cultural_contexts = set([item[\"context\"] for item in self.culture_specific_data])\n",
    "        \n",
    "        relevant_contexts = []\n",
    "        for context in cultural_contexts:\n",
    "            if context in prompt.lower():\n",
    "                relevant_contexts.append(context)\n",
    "        \n",
    "        if not relevant_contexts:\n",
    "            return generated_text\n",
    "        \n",
    "        # Find relevant cultural expressions for these contexts\n",
    "        relevant_expressions = [\n",
    "            item for item in self.culture_specific_data \n",
    "            if item[\"context\"] in relevant_contexts\n",
    "        ]\n",
    "        enhanced_text = generated_text\n",
    "        for expr in relevant_expressions[:2]:  # Limit to 2 expressions to avoid overloading\n",
    "            if expr[\"expression\"] not in enhanced_text:\n",
    "                # Find suitable place to insert expression\n",
    "                sentences = enhanced_text.split('।')\n",
    "                if len(sentences) > 1:\n",
    "                    insertion_point = len(sentences) // 2\n",
    "                    sentences[insertion_point] = f\"{sentences[insertion_point]} {expr['expression']}\"\n",
    "                    enhanced_text = '।'.join(sentences)\n",
    "        \n",
    "        return enhanced_text\n",
    "    \n",
    "    def enhance_sentiment_analysis(self, text, sentiment_result):\n",
    "        \"\"\"Enhance sentiment analysis with cultural context.\"\"\"\n",
    "        enhanced_result = sentiment_result.copy()\n",
    "        \n",
    "        # Check for cultural expressions that might affect sentiment\n",
    "        for item in self.culture_specific_data:\n",
    "            if item[\"expression\"] in text:\n",
    "                # Add cultural context to analysis\n",
    "                if \"cultural_context\" not in enhanced_result:\n",
    "                    enhanced_result[\"cultural_context\"] = []\n",
    "                \n",
    "                enhanced_result[\"cultural_context\"].append({\n",
    "                    \"expression\": item[\"expression\"],\n",
    "                    \"meaning\": item[\"meaning\"],\n",
    "                    \"context\": item[\"context\"]\n",
    "                })\n",
    "                \n",
    "                # Adjust confidence if necessary\n",
    "                if enhanced_result[\"confidence\"] > 0.8:\n",
    "                    enhanced_result[\"confidence\"] *= 0.95  # Slightly reduce very high confidence\n",
    "                \n",
    "                # Add explanation\n",
    "                if \"explanation\" not in enhanced_result:\n",
    "                    enhanced_result[\"explanation\"] = []\n",
    "                \n",
    "                enhanced_result[\"explanation\"].append(\n",
    "                    f\"Result may be influenced by cultural expression '{item['expression']}'.\"\n",
    "                )\n",
    "        \n",
    "        return enhanced_result\n",
    "#################################\n",
    "# 6. CODE-MIXING HANDLER       #\n",
    "#################################\n",
    "class HinglishProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Hinglish processor for code-mixed text.\"\"\"\n",
    "        self.hindi_tokenizer = None\n",
    "        self.english_tokenizer = None\n",
    "        self.transliteration_map = self._load_transliteration_map()\n",
    "        \n",
    "    def _load_transliteration_map(self):\n",
    "        \"\"\"Load or create transliteration mapping from Roman to Devanagari.\"\"\"\n",
    "        return {\n",
    "            # Basic vowels\n",
    "            'a': 'अ', 'aa': 'आ', 'i': 'इ', 'ee': 'ई', 'u': 'उ', 'oo': 'ऊ',\n",
    "            'e': 'ए', 'ai': 'ऐ', 'o': 'ओ', 'au': 'औ',\n",
    "            \n",
    "            # Consonants\n",
    "            'k': 'क', 'kh': 'ख', 'g': 'ग', 'gh': 'घ', 'ng': 'ङ',\n",
    "            'ch': 'च', 'chh': 'छ', 'j': 'ज', 'jh': 'झ', 'n': 'ञ',\n",
    "            't': 'ट', 'th': 'ठ', 'd': 'ड', 'dh': 'ढ', 'n': 'ण',\n",
    "            'th': 'त', 'thh': 'थ', 'd': 'द', 'dh': 'ध', 'n': 'न',\n",
    "            'p': 'प', 'ph': 'फ', 'b': 'ब', 'bh': 'भ', 'm': 'म',\n",
    "            'y': 'य', 'r': 'र', 'l': 'ल', 'v': 'व', 'sh': 'श',\n",
    "            's': 'स', 'h': 'ह',\n",
    "            \n",
    "            # Common words\n",
    "            'hai': 'है', 'hain': 'हैं', 'kya': 'क्या', 'main': 'मैं',\n",
    "            'aap': 'आप', 'tum': 'तुम', 'yeh': 'यह', 'woh': 'वह',\n",
    "            'aur': 'और', 'par': 'पर', 'mein': 'में', 'se': 'से',\n",
    "            'ka': 'का', 'ki': 'की', 'ke': 'के', 'ko': 'को',\n",
    "            'namaste': 'नमस्ते', 'dhanyavaad': 'धन्यवाद'\n",
    "        }\n",
    "    \n",
    "    def detect_language_word(self, word):\n",
    "        \"\"\"Detect if a word is English, Hindi in Roman script, or Hindi in Devanagari.\"\"\"\n",
    "        if any(0x900 <= ord(c) <= 0x97F for c in word):\n",
    "            return \"hindi_dev\"  # Hindi in Devanagari\n",
    "        \n",
    "        # Check if word is in transliteration map\n",
    "        if word.lower() in self.transliteration_map:\n",
    "            return \"hindi_roman\"  # Hindi in Roman script\n",
    "        \n",
    "        # Basic heuristic for Hindi words in Roman script\n",
    "        hindi_suffixes = ['aa', 'ee', 'oo', 'ai', 'au', 'an', 'en', 'in']\n",
    "        if any(word.lower().endswith(suffix) for suffix in hindi_suffixes):\n",
    "            return \"hindi_roman\"\n",
    "        \n",
    "        return \"english\"  # Default to English\n",
    "    \n",
    "    def transliterate_roman_to_devanagari(self, text):\n",
    "        \"\"\"Transliterate Roman script Hindi to Devanagari.\"\"\"\n",
    "        words = text.split()\n",
    "        transliterated_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            language = self.detect_language_word(word)\n",
    "            \n",
    "            if language == \"hindi_dev\":\n",
    "                transliterated_words.append(word)  # Already Devanagari\n",
    "            elif language == \"hindi_roman\":\n",
    "                if word.lower() in self.transliteration_map:\n",
    "                    transliterated_words.append(self.transliteration_map[word.lower()])\n",
    "                else:\n",
    "                    # Try to match parts of words - simplified approach\n",
    "                    # A real implementation would use a more sophisticated algorithm\n",
    "                    processed_word = word\n",
    "                    for rom, dev in self.transliteration_map.items():\n",
    "                        processed_word = processed_word.replace(rom, dev)\n",
    "                    transliterated_words.append(processed_word)\n",
    "            else:\n",
    "                # Keep English words as is\n",
    "                transliterated_words.append(word)\n",
    "        \n",
    "        return \" \".join(transliterated_words)\n",
    "    \n",
    "    def normalize_hinglish(self, text):\n",
    "        \"\"\"Normalize Hinglish text for better processing.\"\"\"\n",
    "        # Transliterate Hindi words in Roman script to Devanagari\n",
    "        normalized_text = self.transliterate_roman_to_devanagari(text)\n",
    "        \n",
    "        # Handle English words and punctuation\n",
    "        # Keep this in a more consistent format for downstream processing\n",
    "        \n",
    "        return normalized_text\n",
    "    \n",
    "    def tag_language_tokens(self, text):\n",
    "        \"\"\"Tag each token in the text with its language.\"\"\"\n",
    "        words = text.split()\n",
    "        tagged_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            language = self.detect_language_word(word)\n",
    "            tagged_words.append((word, language))\n",
    "        \n",
    "        return tagged_words\n",
    "    \n",
    "    def preprocess_for_nlp_tasks(self, text):\n",
    "        \"\"\"Preprocess Hinglish text for NLP tasks.\"\"\"\n",
    "        # First, normalize the text\n",
    "        normalized_text = self.normalize_hinglish(text)\n",
    "        \n",
    "        # Tag tokens with languages\n",
    "        tagged_tokens = self.tag_language_tokens(normalized_text)\n",
    "        \n",
    "        # Create two versions of the text:\n",
    "        # 1. Hindi-only (with English words preserved but marked)\n",
    "        # 2. Fully transliterated (all Hindi in Devanagari)\n",
    "        \n",
    "        hindi_only = []\n",
    "        fully_transliterated = []\n",
    "        \n",
    "        for word, lang in tagged_tokens:\n",
    "            if lang == \"english\":\n",
    "                hindi_only.append(f\"[ENG]{word}[/ENG]\")\n",
    "                fully_transliterated.append(word)  # Keep English as is\n",
    "            elif lang == \"hindi_roman\":\n",
    "                if word.lower() in self.transliteration_map:\n",
    "                    dev_word = self.transliteration_map[word.lower()]\n",
    "                    hindi_only.append(dev_word)\n",
    "                    fully_transliterated.append(dev_word)\n",
    "                else:\n",
    "                    hindi_only.append(word)  # Keep as is if not in map\n",
    "                    fully_transliterated.append(word)\n",
    "            else:  # hindi_dev\n",
    "                hindi_only.append(word)\n",
    "                fully_transliterated.append(word)\n",
    "        \n",
    "        return {\n",
    "            \"normalized\": normalized_text,\n",
    "            \"hindi_only\": \" \".join(hindi_only),\n",
    "            \"fully_transliterated\": \" \".join(fully_transliterated),\n",
    "            \"tagged_tokens\": tagged_tokens\n",
    "        }\n",
    "\n",
    "\n",
    "#################################\n",
    "# 7. DOMAIN ADAPTATION MODULE   #\n",
    "#################################\n",
    "\n",
    "class DomainAdapter:\n",
    "    def __init__(self, base_model, model_type=\"translation\"):\n",
    "        \"\"\"\n",
    "        Initialize domain adapter for specialized Hindi NLP tasks.\n",
    "        \n",
    "        Args:\n",
    "            base_model: The base model to adapt (translation, generation, or sentiment)\n",
    "            model_type: Type of model (\"translation\", \"generation\", or \"sentiment\")\n",
    "        \"\"\"\n",
    "        self.base_model = base_model\n",
    "        self.model_type = model_type\n",
    "        self.domain_data = {}\n",
    "    \n",
    "    def load_domain_data(self, domain, path):\n",
    "        \"\"\"Load domain-specific data.\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            if self.model_type == \"translation\":\n",
    "                # Load parallel corpus for the domain\n",
    "                df = pd.read_csv(path, sep='\\t', names=['en', 'hi'])\n",
    "            elif self.model_type == \"generation\":\n",
    "                # Load prompts and completions for the domain\n",
    "                df = pd.read_csv(path)\n",
    "            elif self.model_type == \"sentiment\":\n",
    "                # Load text and sentiment labels for the domain\n",
    "                df = pd.read_csv(path)\n",
    "            \n",
    "            self.domain_data[domain] = df\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Warning: {path} not found. Domain adaptation for {domain} will be limited.\")\n",
    "            return False\n",
    "    \n",
    "    def create_domain_specific_dataset(self, domain):\n",
    "        \"\"\"Create domain-specific dataset for fine-tuning.\"\"\"\n",
    "        if domain not in self.domain_data:\n",
    "            print(f\"Error: Data for domain {domain} not loaded.\")\n",
    "            return None\n",
    "        \n",
    "        df = self.domain_data[domain]\n",
    "        \n",
    "        # Split into train-val\n",
    "        train_df = df.sample(frac=0.8, random_state=42)\n",
    "        val_df = df.drop(train_df.index)\n",
    "        \n",
    "        # Convert to HF datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def augment_domain_data(self, domain):\n",
    "        \"\"\"Augment domain data using basic techniques.\"\"\"\n",
    "        if domain not in self.domain_data:\n",
    "            print(f\"Error: Data for domain {domain} not loaded.\")\n",
    "            return\n",
    "        \n",
    "        df = self.domain_data[domain]\n",
    "        augmented_rows = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if self.model_type == \"translation\":\n",
    "                # For translation: create variations by adding/removing words\n",
    "                en_text = row['en']\n",
    "                hi_text = row['hi']\n",
    "                \n",
    "                # Simple augmentation: add/remove determiners or adjectives\n",
    "                # This is a simplified approach - more sophisticated techniques would be used\n",
    "                augmented_rows.append({\n",
    "                    'en': en_text.replace('the ', ''),\n",
    "                    'hi': hi_text\n",
    "                })\n",
    "                \n",
    "            elif self.model_type == \"generation\":\n",
    "                # For generation: create variations of prompts\n",
    "                prompt = row['prompt']\n",
    "                completion = row['completion']\n",
    "                \n",
    "                # Rephrase prompt slightly\n",
    "                augmented_rows.append({\n",
    "                    'prompt': f\"कृपया {prompt}\",  # Add \"please\" in Hindi\n",
    "                    'completion': completion\n",
    "                })\n",
    "                \n",
    "            elif self.model_type == \"sentiment\":\n",
    "                # For sentiment: simple word replacements/additions\n",
    "                text = row['text']\n",
    "                label = row['label']\n",
    "                \n",
    "                # Add intensifiers for positive/negative sentiments\n",
    "                if label == 2:  # positive\n",
    "                    augmented_rows.append({\n",
    "                        'text': f\"बहुत {text}\",  # Add \"very\" in Hindi\n",
    "                        'label': label\n",
    "                    })\n",
    "                elif label == 0:  # negative\n",
    "                    augmented_rows.append({\n",
    "                        'text': f\"बिल्कुल {text}\",  # Add \"absolutely\" in Hindi\n",
    "                        'label': label\n",
    "                    })\n",
    "        \n",
    "        # Add augmented data to original dataset\n",
    "        augmented_df = pd.DataFrame(augmented_rows)\n",
    "        self.domain_data[domain] = pd.concat([df, augmented_df])\n",
    "    \n",
    "    def fine_tune_domain_model(self, domain, output_dir, epochs=3):\n",
    "        \"\"\"Fine-tune model on domain-specific data.\"\"\"\n",
    "        if domain not in self.domain_data:\n",
    "            print(f\"Error: Data for domain {domain} not loaded.\")\n",
    "            return None\n",
    "        \n",
    "        # Create domain dataset\n",
    "        train_dataset, val_dataset = self.create_domain_specific_dataset(domain)\n",
    "        \n",
    "        # Determine how to fine-tune based on model type\n",
    "        if self.model_type == \"translation\":\n",
    "            trainer = self.base_model.fine_tune(\n",
    "                train_dataset, \n",
    "                val_dataset, \n",
    "                output_dir=f\"{output_dir}/{domain}\", \n",
    "                epochs=epochs\n",
    "            )\n",
    "        elif self.model_type == \"generation\":\n",
    "            trainer, _ = self.base_model.fine_tune(\n",
    "                train_dataset, \n",
    "                val_dataset, \n",
    "                output_dir=f\"{output_dir}/{domain}\", \n",
    "                epochs=epochs\n",
    "            )\n",
    "        elif self.model_type == \"sentiment\":\n",
    "            trainer = self.base_model.fine_tune(\n",
    "                train_dataset, \n",
    "                val_dataset, \n",
    "                output_dir=f\"{output_dir}/{domain}\", \n",
    "                epochs=epochs\n",
    "            )\n",
    "        \n",
    "        print(f\"Domain adaptation for {domain} completed.\")\n",
    "        return trainer\n",
    "\n",
    "\n",
    "#################################\n",
    "# 8. EVALUATION MODULE         #\n",
    "#################################\n",
    "\n",
    "class HindiNLPEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize evaluator for Hindi NLP tasks.\"\"\"\n",
    "        self.metrics = {\n",
    "            \"translation\": [\"bleu\", \"meteor\", \"human_eval\"],\n",
    "            \"generation\": [\"perplexity\", \"human_eval\"],\n",
    "            \"sentiment\": [\"f1\", \"accuracy\", \"precision\", \"recall\"]\n",
    "        }\n",
    "        self.human_evaluators = []  # Would contain evaluator information in real implementation\n",
    "    \n",
    "    def evaluate_translation(self, model, test_dataset, metrics=None):\n",
    "        \"\"\"\n",
    "        Evaluate translation model with specified metrics.\n",
    "        \n",
    "        Args:\n",
    "            model: HindiTranslationModel to evaluate\n",
    "            test_dataset: Test dataset with source and reference translations\n",
    "            metrics: List of metrics to use (defaults to all available)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of evaluation results\n",
    "        \"\"\"\n",
    "        if metrics is None:\n",
    "            metrics = [\"bleu\", \"meteor\"]\n",
    "        \n",
    "        results = {}\n",
    "        sources = test_dataset['en']\n",
    "        references = test_dataset['hi']\n",
    "        \n",
    "        # Generate translations\n",
    "        translations = []\n",
    "        for source in sources:\n",
    "            translation = model.translate(source)\n",
    "            translations.append(translation)\n",
    "        \n",
    "        # Calculate BLEU\n",
    "        if \"bleu\" in metrics:\n",
    "            bleu = corpus_bleu(translations, [references]).score\n",
    "            results[\"bleu\"] = bleu\n",
    "        \n",
    "        # Calculate METEOR for a sample (for efficiency)\n",
    "        if \"meteor\" in metrics:\n",
    "            meteor_scores = []\n",
    "            sample_size = min(50, len(translations))  # Limit sample size\n",
    "            for trans, ref in zip(translations[:sample_size], references[:sample_size]):\n",
    "                tokenized_trans = nltk.word_tokenize(trans)\n",
    "                tokenized_ref = nltk.word_tokenize(ref)\n",
    "                meteor_scores.append(meteor_score([tokenized_ref], tokenized_trans))\n",
    "            \n",
    "            meteor = np.mean(meteor_scores) if meteor_scores else 0\n",
    "            results[\"meteor\"] = meteor\n",
    "        \n",
    "        # Human evaluation would be implemented here in a real system\n",
    "        if \"human_eval\" in metrics and self.human_evaluators:\n",
    "            # Placeholder for human evaluation logic\n",
    "            results[\"human_eval\"] = {\n",
    "                \"fluency\": 0.0,\n",
    "                \"adequacy\": 0.0,\n",
    "                \"idiomaticity\": 0.0\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_generation(self, model, test_prompts, metrics=None):\n",
    "        \"\"\"\n",
    "        Evaluate text generation model.\n",
    "        \n",
    "        Args:\n",
    "            model: HindiTextGenerator to evaluate\n",
    "            test_prompts: List of prompts to generate from\n",
    "            metrics: List of metrics to use\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of evaluation results\n",
    "        \"\"\"\n",
    "        if metrics is None:\n",
    "            metrics = [\"perplexity\"]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Generate text for each prompt\n",
    "        generated_texts = []\n",
    "        for prompt in test_prompts:\n",
    "            generated = model.generate_text(prompt)[0]\n",
    "            generated_texts.append(generated)\n",
    "        \n",
    "        # Calculate perplexity if applicable\n",
    "        if \"perplexity\" in metrics and hasattr(model, \"compute_perplexity\"):\n",
    "            # This would require test dataset in the right format\n",
    "            # results[\"perplexity\"] = model.compute_perplexity(test_dataset)\n",
    "            results[\"perplexity\"] = \"Needs formatted test dataset\"\n",
    "        \n",
    "        # Human evaluation would be implemented here\n",
    "        if \"human_eval\" in metrics and self.human_evaluators:\n",
    "            # Placeholder for human evaluation logic\n",
    "            results[\"human_eval\"] = {\n",
    "                \"fluency\": 0.0,\n",
    "                \"coherence\": 0.0,\n",
    "                \"cultural_relevance\": 0.0\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_sentiment(self, model, test_dataset, metrics=None):\n",
    "        \"\"\"\n",
    "        Evaluate sentiment analysis model.\n",
    "        \n",
    "        Args:\n",
    "            model: HindiSentimentAnalyzer to evaluate\n",
    "            test_dataset: Test dataset with text and labels\n",
    "            metrics: List of metrics to use\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of evaluation results\n",
    "        \"\"\"\n",
    "        if metrics is None:\n",
    "            metrics = [\"f1\", \"accuracy\", \"precision\", \"recall\"]\n",
    "        \n",
    "        results = {}\n",
    "        texts = test_dataset['text']\n",
    "        true_labels = test_dataset['label']\n",
    "        \n",
    "        # Generate predictions\n",
    "        predicted_labels = []\n",
    "        predicted_probs = []\n",
    "        \n",
    "        for text in texts:\n",
    "            prediction = model.predict_sentiment(text)\n",
    "            label_id = model.label2id[prediction[\"sentiment\"]]\n",
    "            predicted_labels.append(label_id)\n",
    "            \n",
    "            # Store probabilities for each class for ROC analysis\n",
    "            probs = [prediction[\"probabilities\"][label] for label in model.id2label.values()]\n",
    "            predicted_probs.append(probs)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if any(metric in [\"f1\", \"precision\", \"recall\"] for metric in metrics):\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                true_labels, predicted_labels, average='weighted'\n",
    "            )\n",
    "            \n",
    "            if \"precision\" in metrics:\n",
    "                results[\"precision\"] = precision\n",
    "            if \"recall\" in metrics:\n",
    "                results[\"recall\"] = recall\n",
    "            if \"f1\" in metrics:\n",
    "                results[\"f1\"] = f1\n",
    "        \n",
    "        if \"accuracy\" in metrics:\n",
    "            accuracy = (np.array(predicted_labels) == np.array(true_labels)).mean()\n",
    "            results[\"accuracy\"] = accuracy\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_across_domains(self, model, domain_datasets, task=\"translation\"):\n",
    "        \"\"\"\n",
    "        Evaluate model performance across different domains.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to evaluate\n",
    "            domain_datasets: Dictionary of domain-specific test datasets\n",
    "            task: Task type (\"translation\", \"generation\", \"sentiment\")\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of evaluation results by domain\n",
    "        \"\"\"\n",
    "        domain_results = {}\n",
    "        \n",
    "        for domain, dataset in domain_datasets.items():\n",
    "            print(f\"Evaluating {task} performance on {domain} domain...\")\n",
    "            \n",
    "            if task == \"translation\":\n",
    "                results = self.evaluate_translation(model, dataset)\n",
    "            elif task == \"generation\":\n",
    "                # For generation, dataset should contain prompts\n",
    "                prompts = dataset['prompt'] if 'prompt' in dataset.column_names else dataset['text']\n",
    "                results = self.evaluate_generation(model, prompts)\n",
    "            elif task == \"sentiment\":\n",
    "                results = self.evaluate_sentiment(model, dataset)\n",
    "            \n",
    "            domain_results[domain] = results\n",
    "        \n",
    "        return domain_results\n",
    "    \n",
    "    def evaluate_cultural_context(self, model, cultural_test_set, task=\"translation\"):\n",
    "        \"\"\"\n",
    "        Specifically evaluate model's handling of cultural context.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to evaluate\n",
    "            cultural_test_set: Test set rich in cultural references\n",
    "            task: Task type\n",
    "        \n",
    "        Returns:\n",
    "            Evaluation results focused on cultural aspects\n",
    "        \"\"\"\n",
    "        # This would be implemented with specific cultural evaluation metrics\n",
    "        # Placeholder implementation\n",
    "        print(\"Evaluating cultural context handling...\")\n",
    "        \n",
    "        if task == \"translation\":\n",
    "            results = self.evaluate_translation(model, cultural_test_set)\n",
    "        elif task == \"generation\":\n",
    "            prompts = cultural_test_set['prompt'] if 'prompt' in cultural_test_set.column_names else cultural_test_set['text']\n",
    "            results = self.evaluate_generation(model, prompts)\n",
    "        elif task == \"sentiment\":\n",
    "            results = self.evaluate_sentiment(model, cultural_test_set)\n",
    "        \n",
    "        # Additional cultural-specific metrics would be added here\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "#################################\n",
    "# 9. MAIN APPLICATION          #\n",
    "#################################\n",
    "\n",
    "class HindiNLPApp:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Hindi NLP application framework.\"\"\"\n",
    "        self.pipeline = HindiNLPPipeline()\n",
    "        self.hinglish_processor = HinglishProcessor()\n",
    "        self.evaluator = HindiNLPEvaluator()\n",
    "        self.cultural_adapter = None\n",
    "        self.domain_adapters = {}\n",
    "    \n",
    "    def initialize_models(self, translation_model=\"ai4bharat/IndicBART\", \n",
    "                          generation_model=\"ai4bharat/IndicBART\",\n",
    "                          sentiment_model=\"ai4bharat/MuRIL-base\"):\n",
    "        \"\"\"Initialize all models.\"\"\"\n",
    "        print(\"Initializing translation model...\")\n",
    "        self.pipeline.initialize_translation(translation_model)\n",
    "        \n",
    "        print(\"Initializing text generation model...\")\n",
    "        self.pipeline.initialize_generation(generation_model)\n",
    "        \n",
    "        print(\"Initializing sentiment analysis model...\")\n",
    "        self.pipeline.initialize_sentiment(sentiment_model)\n",
    "        \n",
    "        # Initialize cultural adaptation for all models\n",
    "        self.cultural_adapter = CulturalAdaptation(self.pipeline)\n",
    "        self.cultural_adapter.load_cultural_expressions(\"path/to/cultural_expressions.csv\")\n",
    "        \n",
    "        # Initialize domain adapters\n",
    "        self.domain_adapters[\"translation\"] = DomainAdapter(\n",
    "            self.pipeline.translation_model, model_type=\"translation\"\n",
    "        )\n",
    "        self.domain_adapters[\"generation\"] = DomainAdapter(\n",
    "            self.pipeline.generation_model, model_type=\"generation\"\n",
    "        )\n",
    "        self.domain_adapters[\"sentiment\"] = DomainAdapter(\n",
    "            self.pipeline.sentiment_model, model_type=\"sentiment\"\n",
    "        )\n",
    "        \n",
    "        print(\"All models initialized successfully.\")\n",
    "    \n",
    "    def load_domain_datasets(self, domains=[\"legal\", \"technical\", \"medical\"]):\n",
    "        \"\"\"Load domain-specific datasets for all tasks.\"\"\"\n",
    "        for domain in domains:\n",
    "            # Paths would be configured properly in real implementation\n",
    "            print(f\"Loading {domain} domain datasets...\")\n",
    "            \n",
    "            # For translation\n",
    "            self.domain_adapters[\"translation\"].load_domain_data(\n",
    "                domain, f\"data/domains/{domain}/translation.csv\"\n",
    "            )\n",
    "            \n",
    "            # For generation\n",
    "            self.domain_adapters[\"generation\"].load_domain_data(\n",
    "                domain, f\"data/domains/{domain}/generation.csv\"\n",
    "            )\n",
    "            \n",
    "            # For sentiment\n",
    "            self.domain_adapters[\"sentiment\"].load_domain_data(\n",
    "                domain, f\"data/domains/{domain}/sentiment.csv\"\n",
    "            )\n",
    "    \n",
    "    def fine_tune_all_models(self, output_dir=\"models/fine_tuned\"):\n",
    "        \"\"\"Fine-tune all models on available datasets.\"\"\"\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Fine-tune translation model\n",
    "        print(\"Fine-tuning translation model...\")\n",
    "        if self.pipeline.translation_model:\n",
    "            train_dataset, val_dataset, _ = self.pipeline.translation_model.load_iitb_corpus(\n",
    "                \"data/iitb_corpus.csv\", sample_size=10000\n",
    "            )\n",
    "            self.pipeline.translation_model.fine_tune(\n",
    "                train_dataset, val_dataset, f\"{output_dir}/translation\"\n",
    "            )\n",
    "        \n",
    "        # Fine-tune generation model\n",
    "        print(\"Fine-tuning text generation model...\")\n",
    "        if self.pipeline.generation_model:\n",
    "            train_dataset, val_dataset = self.pipeline.generation_model.load_hindi_generation_data(\n",
    "                \"data/hindi_generation.csv\"\n",
    "            )\n",
    "            self.pipeline.generation_model.fine_tune(\n",
    "                train_dataset, val_dataset, f\"{output_dir}/generation\"\n",
    "            )\n",
    "        \n",
    "        # Fine-tune sentiment model\n",
    "        print(\"Fine-tuning sentiment analysis model...\")\n",
    "        if self.pipeline.sentiment_model:\n",
    "            train_dataset, val_dataset, _ = self.pipeline.sentiment_model.load_hindi_sentiment_data(\n",
    "                \"data/hindi_sentiment.csv\"\n",
    "            )\n",
    "            self.pipeline.sentiment_model.fine_tune(\n",
    "                train_dataset, val_dataset, f\"{output_dir}/sentiment\"\n",
    "            )\n",
    "        \n",
    "        print(\"Basic fine-tuning completed for all models.\")\n",
    "    \n",
    "    def fine_tune_domain_models(self, domains=[\"legal\", \"technical\", \"medical\"], \n",
    "                               output_dir=\"models/domain_adapted\"):\n",
    "        \"\"\"Fine-tune models for specific domains.\"\"\"\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for domain in domains:\n",
    "            print(f\"Fine-tuning models for {domain} domain...\")\n",
    "            \n",
    "            # Fine-tune translation for domain\n",
    "            if \"translation\" in self.domain_adapters:\n",
    "                self.domain_adapters[\"translation\"].fine_tune_domain_model(\n",
    "                    domain, f\"{output_dir}/translation\"\n",
    "                )\n",
    "            \n",
    "            # Fine-tune generation for domain\n",
    "            if \"generation\" in self.domain_adapters:\n",
    "                self.domain_adapters[\"generation\"].fine_tune_domain_model(\n",
    "                    domain, f\"{output_dir}/generation\"\n",
    "                )\n",
    "            \n",
    "            # Fine-tune sentiment for domain\n",
    "            if \"sentiment\" in self.domain_adapters:\n",
    "                self.domain_adapters[\"sentiment\"].fine_tune_domain_model(\n",
    "                    domain, f\"{output_dir}/sentiment\"\n",
    "                )\n",
    "    \n",
    "    def evaluate_all_models(self, test_datasets):\n",
    "        \"\"\"Evaluate all models on test datasets.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Evaluate translation model\n",
    "        if self.pipeline.translation_model and \"translation\" in test_datasets:\n",
    "            print(\"Evaluating translation model...\")\n",
    "            results[\"translation\"] = self.evaluator.evaluate_translation(\n",
    "                self.pipeline.translation_model, test_datasets[\"translation\"]\n",
    "            )\n",
    "        \n",
    "        # Evaluate generation model\n",
    "        if self.pipeline.generation_model and \"generation\" in test_datasets:\n",
    "            print(\"Evaluating text generation model...\")\n",
    "            prompts = test_datasets[\"generation\"]['prompt'] if 'prompt' in test_datasets[\"generation\"].column_names else test_datasets[\"generation\"]['text']\n",
    "            results[\"generation\"] = self.evaluator.evaluate_generation(\n",
    "                self.pipeline.generation_model, prompts\n",
    "            )\n",
    "        \n",
    "        # Evaluate sentiment model\n",
    "        if self.pipeline.sentiment_model and \"sentiment\" in test_datasets:\n",
    "            print(\"Evaluating sentiment analysis model...\")\n",
    "            results[\"sentiment\"] = self.evaluator.evaluate_sentiment(\n",
    "                self.pipeline.sentiment_model, test_datasets[\"sentiment\"]\n",
    "            )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_text(self, text, task=\"all\"):\n",
    "        \"\"\"Process text using the appropriate pipeline based on task.\"\"\"\n",
    "        # First, check if text is Hinglish and process if needed\n",
    "        if any(0x900 <= ord(c) <= 0x97F for c in text) and any(ord(c) < 128 for c in text):\n",
    "            print(\"Detected code-mixed text (Hinglish), preprocessing...\")\n",
    "            processed = self.hinglish_processor.preprocess_for_nlp_tasks(text)\n",
    "            text = processed[\"hindi_only\"]  # Use Hindi-only version\n",
    "        \n",
    "        # Process with the main pipeline\n",
    "        results = self.pipeline.end_to_end_processing(text, task)\n",
    "        \n",
    "        # Enhanced with cultural context if applicable\n",
    "        if task in [\"translate\", \"all\"] and \"translation\" in results:\n",
    "            results[\"translation_with_cultural_context\"] = self.cultural_adapter.enhance_translation(\n",
    "                text, results[\"translation\"]\n",
    "            )\n",
    "        \n",
    "        if task in [\"generate\", \"all\"] and \"generated_text\" in results:\n",
    "            results[\"generated_text_with_cultural_context\"] = self.cultural_adapter.enhance_text_generation(\n",
    "                text, results[\"generated_text\"][0]\n",
    "            )\n",
    "        \n",
    "        if task in [\"sentiment\", \"all\"] and \"sentiment_analysis\" in results:\n",
    "            results[\"sentiment_with_cultural_context\"] = self.cultural_adapter.enhance_sentiment_analysis(\n",
    "                text, results[\"sentiment_analysis\"]\n",
    "            )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def demo(self):\n",
    "        \"\"\"Run a demonstration of the Hindi NLP capabilities.\"\"\"\n",
    "        print(\"Hindi NLP Framework Demonstration\")\n",
    "        print(\"================================\\n\")\n",
    "        \n",
    "        # 1. Translation demo\n",
    "        print(\"1. Machine Translation Demo\")\n",
    "        english_texts = [\n",
    "            \"Hello, how are you?\",\n",
    "            \"India is a diverse country with many cultures and languages.\",\n",
    "            \"The weather is quite pleasant today.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nEnglish to Hindi Translation:\")\n",
    "        for text in english_texts:\n",
    "            translation = self.pipeline.translate(text)\n",
    "            print(f\"EN: {text}\")\n",
    "            print(f\"HI: {translation}\\n\")\n",
    "        \n",
    "        # 2. Text Generation demo\n",
    "        print(\"\\n2. Hindi Text Generation Demo\")\n",
    "        hindi_prompts = [\n",
    "            \"भारत की संस्कृति के बारे में लिखें।\",\n",
    "            \"दिल्ली शहर का वर्णन करें।\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nHindi Text Generation:\")\n",
    "        for prompt in hindi_prompts:\n",
    "            generated = self.pipeline.generate(prompt)[0]\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Generated: {generated[:200]}...\\n\")\n",
    "        \n",
    "        # 3. Sentiment Analysis demo\n",
    "# Complete the Sentiment Analysis Demo section\n",
    "        hindi_texts = [\n",
    "            \"यह फिल्म बहुत अच्छी थी, मुझे बहुत पसंद आई।\",\n",
    "            \"सेवा बहुत खराब थी और खाना भी ठंडा था।\",\n",
    "            \"इस उत्पाद के बारे में मेरी कोई विशेष राय नहीं है।\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nHindi Sentiment Analysis:\")\n",
    "        for text in hindi_texts:\n",
    "            sentiment = self.pipeline.analyze_sentiment(text)\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Sentiment: {sentiment['sentiment']}, Confidence: {sentiment['confidence']:.2f}\\n\")\n",
    "        \n",
    "        # 4. Hinglish Processing demo\n",
    "        print(\"\\n4. Hinglish Processing Demo\")\n",
    "        hinglish_texts = [\n",
    "            \"Main kal movie dekhne gaya tha and it was amazing!\",\n",
    "            \"Kya aap mujhe bata sakte hain ki yeh kaise kaam karta hai?\",\n",
    "            \"The weather aaj bahut acha hai.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nHinglish Processing:\")\n",
    "        for text in hinglish_texts:\n",
    "            processed = self.hinglish_processor.preprocess_for_nlp_tasks(text)\n",
    "            print(f\"Original: {text}\")\n",
    "            print(f\"Normalized: {processed['normalized']}\")\n",
    "            print(f\"Hindi Only: {processed['hindi_only']}\")\n",
    "            print(f\"Fully Transliterated: {processed['fully_transliterated']}\\n\")\n",
    "        \n",
    "        # 5. Cultural Adaptation demo\n",
    "        print(\"\\n5. Cultural Adaptation Demo\")\n",
    "        cultural_texts = [\n",
    "            \"During Diwali, people light lamps and celebrate with family.\",\n",
    "            \"The wedding ceremony was followed by a grand reception.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nCultural Context Enhancement:\")\n",
    "        for text in cultural_texts:\n",
    "            translation = self.pipeline.translate(text)\n",
    "            enhanced = self.cultural_adapter.enhance_translation(text, translation)\n",
    "            print(f\"Original: {text}\")\n",
    "            print(f\"Basic Translation: {translation}\")\n",
    "            print(f\"Culturally Enhanced: {enhanced}\\n\")\n",
    "        \n",
    "        print(\"\\nDemonstration completed successfully!\")\n",
    "\n",
    "    def run_batch_processing(self, input_file, output_file, task=\"translate\"):\n",
    "        \"\"\"Process a batch of texts from a file.\"\"\"\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"Error: Input file {input_file} not found.\")\n",
    "            return False\n",
    "        \n",
    "        # Read input file\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Process each line\n",
    "        results = []\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                print(f\"Processing line {i+1}/{len(lines)}...\")\n",
    "                result = self.process_text(line, task)\n",
    "                results.append(result)\n",
    "        \n",
    "        # Write results to output file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for i, result in enumerate(results):\n",
    "                f.write(f\"Input {i+1}: {lines[i].strip()}\\n\")\n",
    "                \n",
    "                if task == \"translate\" or task == \"all\":\n",
    "                    if \"translation\" in result:\n",
    "                        f.write(f\"Translation: {result['translation']}\\n\")\n",
    "                    if \"translation_with_cultural_context\" in result:\n",
    "                        f.write(f\"Enhanced Translation: {result['translation_with_cultural_context']}\\n\")\n",
    "                \n",
    "                if task == \"generate\" or task == \"all\":\n",
    "                    if \"generated_text\" in result:\n",
    "                        f.write(f\"Generated Text: {result['generated_text'][0][:200]}...\\n\")\n",
    "                \n",
    "                if task == \"sentiment\" or task == \"all\":\n",
    "                    if \"sentiment_analysis\" in result:\n",
    "                        sentiment = result[\"sentiment_analysis\"]\n",
    "                        f.write(f\"Sentiment: {sentiment['sentiment']}, Confidence: {sentiment['confidence']:.2f}\\n\")\n",
    "                \n",
    "                f.write(\"\\n\" + \"-\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        print(f\"Batch processing completed. Results saved to {output_file}\")\n",
    "        return True\n",
    "\n",
    "    def save_models(self, output_dir=\"models/saved\"):\n",
    "        \"\"\"Save all trained models.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save translation model\n",
    "        if self.pipeline.translation_model:\n",
    "            print(\"Saving translation model...\")\n",
    "            self.pipeline.translation_model.save_model(f\"{output_dir}/translation\")\n",
    "        \n",
    "        # Save generation model\n",
    "        if self.pipeline.generation_model:\n",
    "            print(\"Saving text generation model...\")\n",
    "            self.pipeline.generation_model.save_model(f\"{output_dir}/generation\")\n",
    "        \n",
    "        # Save sentiment model\n",
    "        if self.pipeline.sentiment_model:\n",
    "            print(\"Saving sentiment analysis model...\")\n",
    "            self.pipeline.sentiment_model.save_model(f\"{output_dir}/sentiment\")\n",
    "        \n",
    "        print(f\"All models saved to {output_dir}\")\n",
    "\n",
    "    def load_models(self, input_dir=\"models/saved\"):\n",
    "        \"\"\"Load saved models.\"\"\"\n",
    "        # Load translation model\n",
    "        if os.path.exists(f\"{input_dir}/translation\"):\n",
    "            print(\"Loading translation model...\")\n",
    "            self.pipeline.translation_model.load_model(f\"{input_dir}/translation\")\n",
    "        \n",
    "        # Load generation model\n",
    "        if os.path.exists(f\"{input_dir}/generation\"):\n",
    "            print(\"Loading text generation model...\")\n",
    "            self.pipeline.generation_model.load_model(f\"{input_dir}/generation\")\n",
    "        \n",
    "        # Load sentiment model\n",
    "        if os.path.exists(f\"{input_dir}/sentiment\"):\n",
    "            print(\"Loading sentiment analysis model...\")\n",
    "            self.pipeline.sentiment_model.load_model(f\"{input_dir}/sentiment\")\n",
    "        \n",
    "        print(\"Models loaded successfully.\")\n",
    "\n",
    "\n",
    "#################################\n",
    "# 10. API INTEGRATION MODULE   #\n",
    "#################################\n",
    "\n",
    "class HindiNLPAPI:\n",
    "    def __init__(self, app):\n",
    "        \"\"\"Initialize API with reference to main application.\"\"\"\n",
    "        self.app = app\n",
    "    \n",
    "    def translate_text(self, text, source_language=\"en\", target_language=\"hi\", domain=None):\n",
    "        \"\"\"API endpoint for translation.\"\"\"\n",
    "        response = {\n",
    "            \"input_text\": text,\n",
    "            \"source_language\": source_language,\n",
    "            \"target_language\": target_language,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check if domain-specific translation is requested\n",
    "            if domain and domain in self.app.domain_adapters[\"translation\"].domain_data:\n",
    "                # Use domain-adapted model if available\n",
    "                # In a real implementation, would load domain model\n",
    "                print(f\"Using domain-adapted model for {domain}\")\n",
    "            \n",
    "            # Process translation\n",
    "            result = self.app.process_text(text, task=\"translate\")\n",
    "            \n",
    "            if \"translation\" in result:\n",
    "                response[\"translation\"] = result[\"translation\"]\n",
    "            \n",
    "            if \"translation_with_cultural_context\" in result:\n",
    "                response[\"enhanced_translation\"] = result[\"translation_with_cultural_context\"]\n",
    "            \n",
    "            response[\"status\"] = \"success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            response[\"status\"] = \"error\"\n",
    "            response[\"error\"] = str(e)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def generate_text(self, prompt, max_length=100, num_return=1, domain=None):\n",
    "        \"\"\"API endpoint for text generation.\"\"\"\n",
    "        response = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_length\": max_length,\n",
    "            \"num_return\": num_return\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check if domain-specific generation is requested\n",
    "            if domain and domain in self.app.domain_adapters[\"generation\"].domain_data:\n",
    "                # Use domain-adapted model if available\n",
    "                print(f\"Using domain-adapted model for {domain}\")\n",
    "            \n",
    "            # Process generation\n",
    "            result = self.app.process_text(prompt, task=\"generate\")\n",
    "            \n",
    "            if \"generated_text\" in result:\n",
    "                response[\"generated_text\"] = result[\"generated_text\"]\n",
    "            \n",
    "            if \"generated_text_with_cultural_context\" in result:\n",
    "                response[\"enhanced_generated_text\"] = result[\"generated_text_with_cultural_context\"]\n",
    "            \n",
    "            response[\"status\"] = \"success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            response[\"status\"] = \"error\"\n",
    "            response[\"error\"] = str(e)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def analyze_sentiment(self, text, include_probabilities=False, domain=None):\n",
    "        \"\"\"API endpoint for sentiment analysis.\"\"\"\n",
    "        response = {\n",
    "            \"input_text\": text,\n",
    "            \"include_probabilities\": include_probabilities\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check if domain-specific sentiment analysis is requested\n",
    "            if domain and domain in self.app.domain_adapters[\"sentiment\"].domain_data:\n",
    "                # Use domain-adapted model if available\n",
    "                print(f\"Using domain-adapted model for {domain}\")\n",
    "            \n",
    "            # Process sentiment analysis\n",
    "            result = self.app.process_text(text, task=\"sentiment\")\n",
    "            \n",
    "            if \"sentiment_analysis\" in result:\n",
    "                sentiment = result[\"sentiment_analysis\"]\n",
    "                response[\"sentiment\"] = sentiment[\"sentiment\"]\n",
    "                response[\"confidence\"] = sentiment[\"confidence\"]\n",
    "                \n",
    "                if include_probabilities and \"probabilities\" in sentiment:\n",
    "                    response[\"probabilities\"] = sentiment[\"probabilities\"]\n",
    "            \n",
    "            if \"sentiment_with_cultural_context\" in result:\n",
    "                response[\"enhanced_sentiment\"] = result[\"sentiment_with_cultural_context\"]\n",
    "            \n",
    "            response[\"status\"] = \"success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            response[\"status\"] = \"error\"\n",
    "            response[\"error\"] = str(e)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def process_hinglish(self, text, task=\"translate\"):\n",
    "        \"\"\"API endpoint for processing Hinglish text.\"\"\"\n",
    "        response = {\n",
    "            \"input_text\": text,\n",
    "            \"task\": task\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # First preprocess Hinglish\n",
    "            processed = self.app.hinglish_processor.preprocess_for_nlp_tasks(text)\n",
    "            response[\"preprocessed\"] = processed\n",
    "            \n",
    "            # Then perform the requested task\n",
    "            result = self.app.process_text(processed[\"hindi_only\"], task)\n",
    "            \n",
    "            # Add task-specific results\n",
    "            if task == \"translate\" and \"translation\" in result:\n",
    "                response[\"translation\"] = result[\"translation\"]\n",
    "            \n",
    "            if task == \"generate\" and \"generated_text\" in result:\n",
    "                response[\"generated_text\"] = result[\"generated_text\"]\n",
    "            \n",
    "            if task == \"sentiment\" and \"sentiment_analysis\" in result:\n",
    "                response[\"sentiment\"] = result[\"sentiment_analysis\"]\n",
    "            \n",
    "            response[\"status\"] = \"success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            response[\"status\"] = \"error\"\n",
    "            response[\"error\"] = str(e)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def full_pipeline(self, text):\n",
    "        \"\"\"API endpoint for running full pipeline on text.\"\"\"\n",
    "        response = {\n",
    "            \"input_text\": text\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Run all NLP tasks\n",
    "            result = self.app.process_text(text, task=\"all\")\n",
    "            response.update(result)\n",
    "            response[\"status\"] = \"success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            response[\"status\"] = \"error\"\n",
    "            response[\"error\"] = str(e)\n",
    "        \n",
    "        return response\n",
    "\n",
    "\n",
    "#################################\n",
    "# 11. CONFIG AND UTILITIES     #\n",
    "#################################\n",
    "\n",
    "class HindiNLPConfig:\n",
    "    \"\"\"Configuration class for Hindi NLP Framework.\"\"\"\n",
    "    \n",
    "    DEFAULT_CONFIG = {\n",
    "        \"models\": {\n",
    "            \"translation\": {\n",
    "                \"path\": \"ai4bharat/IndicBART\",\n",
    "                \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                \"batch_size\": 16\n",
    "            },\n",
    "            \"generation\": {\n",
    "                \"path\": \"ai4bharat/IndicBART\",\n",
    "                \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                \"batch_size\": 8,\n",
    "                \"max_length\": 100\n",
    "            },\n",
    "            \"sentiment\": {\n",
    "                \"path\": \"ai4bharat/MuRIL-base\",\n",
    "                \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                \"batch_size\": 32\n",
    "            }\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"translation\": {\n",
    "                \"train\": \"data/iitb_corpus_train.csv\",\n",
    "                \"val\": \"data/iitb_corpus_val.csv\",\n",
    "                \"test\": \"data/iitb_corpus_test.csv\"\n",
    "            },\n",
    "            \"generation\": {\n",
    "                \"train\": \"data/hindi_generation_train.csv\",\n",
    "                \"val\": \"data/hindi_generation_val.csv\",\n",
    "                \"test\": \"data/hindi_generation_test.csv\"\n",
    "            },\n",
    "            \"sentiment\": {\n",
    "                \"train\": \"data/hindi_sentiment_train.csv\",\n",
    "                \"val\": \"data/hindi_sentiment_val.csv\",\n",
    "                \"test\": \"data/hindi_sentiment_test.csv\"\n",
    "            }\n",
    "        },\n",
    "        \"domains\": [\"general\", \"legal\", \"technical\", \"medical\", \"entertainment\"],\n",
    "        \"output_dir\": \"models/fine_tuned\",\n",
    "        \"domain_output_dir\": \"models/domain_adapted\",\n",
    "        \"logging\": {\n",
    "            \"level\": \"INFO\",\n",
    "            \"file\": \"hindi_nlp.log\"\n",
    "        },\n",
    "        \"api\": {\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": 8000,\n",
    "            \"debug\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config_file=None):\n",
    "        \"\"\"Initialize configuration, optionally from file.\"\"\"\n",
    "        self.config = self.DEFAULT_CONFIG\n",
    "        \n",
    "        if config_file and os.path.exists(config_file):\n",
    "            self.load_config(config_file)\n",
    "    \n",
    "    def load_config(self, config_file):\n",
    "        \"\"\"Load configuration from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(config_file, 'r') as f:\n",
    "                loaded_config = json.load(f)\n",
    "                \n",
    "            # Update config with loaded values\n",
    "            self._update_dict(self.config, loaded_config)\n",
    "            print(f\"Configuration loaded from {config_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading configuration: {str(e)}\")\n",
    "    \n",
    "    def _update_dict(self, d, u):\n",
    "        \"\"\"Recursively update nested dictionary.\"\"\"\n",
    "        for k, v in u.items():\n",
    "            if isinstance(v, dict) and k in d and isinstance(d[k], dict):\n",
    "                self._update_dict(d[k], v)\n",
    "            else:\n",
    "                d[k] = v\n",
    "    \n",
    "    def save_config(self, config_file):\n",
    "        \"\"\"Save current configuration to JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(config_file, 'w') as f:\n",
    "                json.dump(self.config, f, indent=2)\n",
    "            print(f\"Configuration saved to {config_file}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving configuration: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        \"\"\"Get configuration value by key path.\"\"\"\n",
    "        keys = key.split('.')\n",
    "        value = self.config\n",
    "        \n",
    "        try:\n",
    "            for k in keys:\n",
    "                value = value[k]\n",
    "            return value\n",
    "        except (KeyError, TypeError):\n",
    "            return default\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        \"\"\"Set configuration value by key path.\"\"\"\n",
    "        keys = key.split('.')\n",
    "        d = self.config\n",
    "        \n",
    "        for k in keys[:-1]:\n",
    "            if k not in d or not isinstance(d[k], dict):\n",
    "                d[k] = {}\n",
    "            d = d[k]\n",
    "        \n",
    "        d[keys[-1]] = value\n",
    "\n",
    "\n",
    "class HindiNLPLogger:\n",
    "    \"\"\"Logging class for Hindi NLP Framework.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize logger with configuration.\"\"\"\n",
    "        self.config = config\n",
    "        self.logger = self._setup_logger()\n",
    "    \n",
    "    def _setup_logger(self):\n",
    "        \"\"\"Set up logging configuration.\"\"\"\n",
    "        logger = logging.getLogger(\"HindiNLP\")\n",
    "        \n",
    "        # Get log level from config\n",
    "        level_str = self.config.get(\"logging.level\", \"INFO\")\n",
    "        level = getattr(logging, level_str)\n",
    "        \n",
    "        logger.setLevel(level)\n",
    "        \n",
    "        # Create console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(level)\n",
    "        \n",
    "        # Create file handler if specified\n",
    "        log_file = self.config.get(\"logging.file\")\n",
    "        if log_file:\n",
    "            os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "            file_handler = logging.FileHandler(log_file)\n",
    "            file_handler.setLevel(level)\n",
    "            \n",
    "            # Create formatter\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            )\n",
    "            file_handler.setFormatter(formatter)\n",
    "            \n",
    "            # Add handler to logger\n",
    "            logger.addHandler(file_handler)\n",
    "        \n",
    "        # Add console handler\n",
    "        logger.addHandler(console_handler)\n",
    "        \n",
    "        return logger\n",
    "    \n",
    "    def info(self, message):\n",
    "        \"\"\"Log info message.\"\"\"\n",
    "        self.logger.info(message)\n",
    "    \n",
    "    def warning(self, message):\n",
    "        \"\"\"Log warning message.\"\"\"\n",
    "        self.logger.warning(message)\n",
    "    \n",
    "    def error(self, message):\n",
    "        \"\"\"Log error message.\"\"\"\n",
    "        self.logger.error(message)\n",
    "    \n",
    "    def debug(self, message):\n",
    "        \"\"\"Log debug message.\"\"\"\n",
    "        self.logger.debug(message)\n",
    "    \n",
    "    def critical(self, message):\n",
    "        \"\"\"Log critical message.\"\"\"\n",
    "        self.logger.critical(message)\n",
    "\n",
    "\n",
    "#################################\n",
    "# 12. COMMAND LINE INTERFACE   #\n",
    "#################################\n",
    "\n",
    "class HindiNLPCLI:\n",
    "    \"\"\"Command Line Interface for Hindi NLP Framework.\"\"\"\n",
    "    \n",
    "    def __init__(self, app):\n",
    "        \"\"\"Initialize CLI with reference to main application.\"\"\"\n",
    "        self.app = app\n",
    "    \n",
    "    def parse_args(self):\n",
    "        \"\"\"Parse command line arguments.\"\"\"\n",
    "        parser = argparse.ArgumentParser(description=\"Hindi NLP Framework CLI\")\n",
    "        \n",
    "        # Main subparsers\n",
    "        subparsers = parser.add_subparsers(dest=\"command\", help=\"Command\")\n",
    "        \n",
    "        # Translation command\n",
    "        translate_parser = subparsers.add_parser(\"translate\", help=\"Translate text\")\n",
    "        translate_parser.add_argument(\"text\", help=\"Text to translate\")\n",
    "        translate_parser.add_argument(\"--src\", default=\"en\", help=\"Source language (default: en)\")\n",
    "        translate_parser.add_argument(\"--tgt\", default=\"hi\", help=\"Target language (default: hi)\")\n",
    "        translate_parser.add_argument(\"--cultural\", action=\"store_true\", help=\"Use cultural enhancement\")\n",
    "        translate_parser.add_argument(\"--domain\", help=\"Specify domain for translation\")\n",
    "        \n",
    "        # Generation command\n",
    "        generate_parser = subparsers.add_parser(\"generate\", help=\"Generate text\")\n",
    "        generate_parser.add_argument(\"prompt\", help=\"Prompt for text generation\")\n",
    "        generate_parser.add_argument(\"--length\", type=int, default=100, help=\"Maximum length (default: 100)\")\n",
    "        generate_parser.add_argument(\"--num\", type=int, default=1, help=\"Number of generations (default: 1)\")\n",
    "        generate_parser.add_argument(\"--cultural\", action=\"store_true\", help=\"Use cultural enhancement\")\n",
    "        generate_parser.add_argument(\"--domain\", help=\"Specify domain for generation\")\n",
    "        \n",
    "        # Sentiment command\n",
    "        sentiment_parser = subparsers.add_parser(\"sentiment\", help=\"Analyze sentiment\")\n",
    "        sentiment_parser.add_argument(\"text\", help=\"Text for sentiment analysis\")\n",
    "        sentiment_parser.add_argument(\"--probs\", action=\"store_true\", help=\"Include class probabilities\")\n",
    "        sentiment_parser.add_argument(\"--cultural\", action=\"store_true\", help=\"Use cultural enhancement\")\n",
    "        sentiment_parser.add_argument(\"--domain\", help=\"Specify domain for sentiment analysis\")\n",
    "        \n",
    "        # Hinglish command\n",
    "        hinglish_parser = subparsers.add_parser(\"hinglish\", help=\"Process Hinglish text\")\n",
    "        hinglish_parser.add_argument(\"text\", help=\"Hinglish text to process\")\n",
    "        hinglish_parser.add_argument(\"--task\", choices=[\"translate\", \"generate\", \"sentiment\", \"all\"],\n",
    "                                     default=\"all\", help=\"Task to perform on processed text\")\n",
    "        \n",
    "        # Batch processing command\n",
    "        batch_parser = subparsers.add_parser(\"batch\", help=\"Process batch of texts\")\n",
    "        batch_parser.add_argument(\"input_file\", help=\"Input file with texts\")\n",
    "        batch_parser.add_argument(\"output_file\", help=\"Output file for results\")\n",
    "        batch_parser.add_argument(\"--task\", choices=[\"translate\", \"generate\", \"sentiment\", \"all\"],\n",
    "                                 default=\"translate\", help=\"Task to perform (default: translate)\")\n",
    "        \n",
    "        # Demo command\n",
    "        subparsers.add_parser(\"demo\", help=\"Run demonstration\")\n",
    "        \n",
    "        # Initialize models command\n",
    "        init_parser = subparsers.add_parser(\"init\", help=\"Initialize models\")\n",
    "        init_parser.add_argument(\"--translation\", help=\"Translation model path\")\n",
    "        init_parser.add_argument(\"--generation\", help=\"Generation model path\")\n",
    "        init_parser.add_argument(\"--sentiment\", help=\"Sentiment model path\")\n",
    "        \n",
    "        # Fine-tune command\n",
    "        finetune_parser = subparsers.add_parser(\"finetune\", help=\"Fine-tune models\")\n",
    "        finetune_parser.add_argument(\"--models\", choices=[\"all\", \"translation\", \"generation\", \"sentiment\"],\n",
    "                                    default=\"all\", help=\"Models to fine-tune (default: all)\")\n",
    "        finetune_parser.add_argument(\"--domains\", nargs=\"+\", help=\"Domains to fine-tune for\")\n",
    "        finetune_parser.add_argument(\"--output\", default=\"models/fine_tuned\", help=\"Output directory\")\n",
    "        \n",
    "        # Evaluate command\n",
    "        evaluate_parser = subparsers.add_parser(\"evaluate\", help=\"Evaluate models\")\n",
    "        evaluate_parser.add_argument(\"--models\", choices=[\"all\", \"translation\", \"generation\", \"sentiment\"],\n",
    "                                    default=\"all\", help=\"Models to evaluate (default: all)\")\n",
    "        evaluate_parser.add_argument(\"--test-data\", help=\"Test data directory\")\n",
    "        \n",
    "        return parser.parse_args()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run CLI application.\"\"\"\n",
    "        args = self.parse_args()\n",
    "        \n",
    "        if args.command == \"translate\":\n",
    "            result = self.app.process_text(args.text, task=\"translate\")\n",
    "            if \"translation\" in result:\n",
    "                print(f\"Translation: {result['translation']}\")\n",
    "            if args.cultural and \"translation_with_cultural_context\" in result:\n",
    "                print(f\"Enhanced Translation: {result['translation_with_cultural_context']}\")\n",
    "        \n",
    "        elif args.command == \"generate\":\n",
    "            result = self.app.process_text(args.prompt, task=\"generate\")\n",
    "            if \"generated_text\" in result:\n",
    "                for i, text in enumerate(result[\"generated_text\"]):\n",
    "                    print(f\"Generated Text {i+1}:\")\n",
    "                    print(text)\n",
    "            if args.cultural and \"generated_text_with_cultural_context\" in result:\n",
    "                print(f\"Enhanced Generation: {result['generated_text_with_cultural_context']}\")\n",
    "        \n",
    "        elif args.command == \"sentiment\":\n",
    "            result = self.app.process_text(args.text, task=\"sentiment\")\n",
    "            if \"sentiment_analysis\" in result:\n",
    "                sentiment = result[\"sentiment_analysis\"]\n",
    "                print(f\"Sentiment: {sentiment['sentiment']}\")\n",
    "                print(f\"Confidence: {sentiment['confidence']:.2f}\")\n",
    "                if args.probs and \"probabilities\" in sentiment:\n",
    "                    print(\"Class Probabilities:\")\n",
    "                    for label, prob in sentiment[\"probabilities\"].items():\n",
    "                        print(f\"  {label}: {prob:.2f}\")\n",
    "            if args.cultural and \"sentiment_with_cultural_context\" in result:\n",
    "                print(f\"Enhanced Sentiment Analysis: {result['sentiment_with_cultural_context']}\")\n",
    "        \n",
    "        elif args.command == \"hinglish\":\n",
    "            processed = self.app.hinglish_processor.preprocess_for_nlp_tasks(args.text)\n",
    "            print(\"Hinglish Processing Results:\")\n",
    "            print(f\"Normalized: {processed['normalized']}\")\n",
    "            print(f\"Hindi Only: {processed['hindi_only']}\")\n",
    "            print(f\"Fully Transliterated: {processed['fully_transliterated']}\")\n",
    "            \n",
    "            if args.task != \"all\":\n",
    "                result = self.app.process_text(processed[\"hindi_only\"], task=args.task)\n",
    "                if args.task == \"translate\" and \"translation\" in result:\n",
    "                    print(f\"\\nTranslation: {result['translation']}\")\n",
    "                elif args.task == \"generate\" and \"generated_text\" in result:\n",
    "                    print(f\"\\nGenerated Text: {result['generated_text'][0]}\")\n",
    "                elif args.task == \"sentiment\" and \"sentiment_analysis\" in result:\n",
    "                    sentiment = result[\"sentiment_analysis\"]\n",
    "                    print(f\"\\nSentiment: {sentiment['sentiment']}\")\n",
    "                    print(f\"Confidence: {sentiment['confidence']:.2f}\")\n",
    "        \n",
    "        elif args.command == \"batch\":\n",
    "            self.app.run_batch_processing(args.input_file, args.output_file, args.task)\n",
    "        \n",
    "        elif args.command == \"demo\":\n",
    "            self.app.demo()\n",
    "        \n",
    "        elif args.command == \"init\":\n",
    "            translation_model = args.translation or \"ai4bharat/IndicBART\"\n",
    "            generation_model = args.generation or \"ai4bharat/IndicBART\"\n",
    "            sentiment_model = args.sentiment or \"ai4bharat/MuRIL-base\"\n",
    "            \n",
    "            self.app.initialize_models(\n",
    "                translation_model=translation_model,\n",
    "                generation_model=generation_model,\n",
    "                sentiment_model=sentiment_model\n",
    "            )\n",
    "        \n",
    "        elif args.command == \"finetune\":\n",
    "            if args.models == \"all\" or args.models == \"translation\":\n",
    "                print(\"Fine-tuning translation model...\")\n",
    "                # Fine-tuning logic\n",
    "            \n",
    "            if args.models == \"all\" or args.models == \"generation\":\n",
    "                print(\"Fine-tuning generation model...\")\n",
    "                # Fine-tuning logic\n",
    "            \n",
    "            if args.models == \"all\" or args.models == \"sentiment\":\n",
    "                print(\"Fine-tuning sentiment model...\")\n",
    "                # Fine-tuning logic\n",
    "            \n",
    "            if args.domains:\n",
    "                print(f\"Fine-tuning for domains: {', '.join(args.domains)}\")\n",
    "                self.app.fine_tune_domain_models(domains=args.domains, output_dir=args.output)\n",
    "        \n",
    "        elif args.command == \"evaluate\":\n",
    "            print(\"Evaluating models...\")\n",
    "            # Evaluation logic based on args.models\n",
    "        \n",
    "        else:\n",
    "            print(\"Please specify a command. Use --help for options.\")\n",
    "\n",
    "\n",
    "#################################\n",
    "# 13. MAIN EXECUTION           #\n",
    "#################################\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the Hindi NLP framework.\"\"\"\n",
    "    # Load configuration\n",
    "    config = HindiNLPConfig(\"config.json\")\n",
    "    \n",
    "    # Set up logging\n",
    "    logger = HindiNLPLogger(config)\n",
    "    logger.info(\"Starting Hindi NLP Framework\")\n",
    "    \n",
    "    # Initialize main application\n",
    "    app = HindiNLPApp()\n",
    "    \n",
    "    # Initialize API\n",
    "    api = HindiNLPAPI(app)\n",
    "    \n",
    "    # Initialize CLI\n",
    "    cli = HindiNLPCLI(app)\n",
    "    \n",
    "    # Parse command line arguments and run\n",
    "    if len(sys.argv) > 1:\n",
    "        cli.run()\n",
    "    else:\n",
    "        # If no arguments, run demo\n",
    "        logger.info(\"No command specified, running demo\")\n",
    "        app.initialize_models()\n",
    "        app.demo()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e78eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0058802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8ccb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
